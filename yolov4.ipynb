{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(\"yolo4_file/yolov4-custom_person_2000.weights\", \"yolo4_file/yolov4-custom_person.cfg\")\n",
    "classes = []\n",
    "with open(\"yolo4_file/person.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "        105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "        118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "        131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "        144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "        157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "        209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "        222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "        235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "        248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
       "        261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "        274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "        287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
       "        300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "        313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "        339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "        352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "        391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403,\n",
       "        404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
       "        417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
       "        430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442,\n",
       "        443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
       "        456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,\n",
       "        469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 480, 481, 482,\n",
       "        483, 932]], dtype=uint16)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from scipy import io\n",
    "mat_file = io.loadmat('data/PRW-v16.04.20/PRW-v16.04.20/ID_train.mat')\n",
    "mat_file['ID_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "mat_file_test = io.loadmat('data/PRW-v16.04.20/PRW-v16.04.20/frame_test.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "img_name = mat_file_test['img_index_test'][6000][0][0]\n",
    "img_path = \"data/PRW-v16.04.20/PRW-v16.04.20/frames/\" + img_name + \".JPG\"\n",
    "img_path2 = \"data/PRW-v16.04.20/PRW-v16.04.20/annotations/\" + img_name + \".jpg.mat\"\n",
    "mat_file_ann = io.loadmat(img_path2)\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "height, width, channels = img.shape\n",
    "print(len(mat_file_ann['box_new']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(img, 0.00392, (608, 608), (0, 0, 0), True, crop=False)\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            # Object detected\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            # 좌표\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "person\n",
      "person\n",
      "person\n",
      "person\n"
     ]
    }
   ],
   "source": [
    "person_list = []\n",
    "\n",
    "person_box = []\n",
    "\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        person_box.append(boxes[i])\n",
    "        label = str(classes[class_ids[i]])\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), colors[0], 2)\n",
    "        print(label)\n",
    "        person = img[y:y+h, x:x+w].copy()\n",
    "        person_list.append(person)\n",
    "        #cv2.putText(img, label, (x, y + 30), font, 3, colors[0], 3)\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = cv2.imread(\"data/PRW-v16.04.20/PRW-v16.04.20/query_box/857_c3s2_118744.JPG\")\n",
    "person_list.insert(0,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candi_folder/candi0.jpg\n",
      "candi_folder/candi1.jpg\n",
      "candi_folder/candi2.jpg\n",
      "candi_folder/candi3.jpg\n",
      "candi_folder/candi4.jpg\n"
     ]
    }
   ],
   "source": [
    "for i, person in enumerate(person_list) :\n",
    "    \n",
    "    path = \"candi_folder/candi\" + str(i) +\".jpg\"\n",
    "    print(path)\n",
    "    cv2.imwrite(path, person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "mat_file = io.loadmat('data/PRW-v16.04.20/PRW-v16.04.20/ID_train.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Tue Feb 23 23:11:59 2016',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'ID_train': array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "          27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "          40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "          53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "          66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "          79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "          92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "         105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "         118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "         131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "         144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "         157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "         170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "         183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "         209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "         222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "         235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "         248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
       "         261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "         274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "         287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
       "         300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "         313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "         326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "         339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "         352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "         365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "         391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403,\n",
       "         404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
       "         417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
       "         430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442,\n",
       "         443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
       "         456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,\n",
       "         469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 480, 481, 482,\n",
       "         483, 932]], dtype=uint16)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "#파일 경로 리스트\n",
    "def make_file_list():\n",
    "    train_img_list = []\n",
    "    for image_path in glob('candi_folder/*.jpg')[:len(person_list)] :\n",
    "        \n",
    "        train_img_list.append(image_path)\n",
    "    \n",
    "    return train_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 불러오고 라벨링하는 클래스\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Custom_Img_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_list, transform = None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_path = self.file_list[index] # 데이터셋에서 파일 하나를 특정\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        user_id = img_path.split('.jpg')[0].split('candi')[1]\n",
    "        \n",
    "        if self.transform == None :\n",
    "            tensor_image = img\n",
    "        else :\n",
    "            tensor_image = self.transform(img)\n",
    "            \n",
    "        return tensor_image, user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "candi_dataset = Custom_Img_Dataset(make_file_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "\n",
    "trainTransform_training  = transforms.Compose([ transforms.Resize((32, 32)), \n",
    "                                               #transforms.Grayscale(num_output_channels=1),\n",
    "                                    transforms.ToTensor()\n",
    "                                    #, transforms.Normalize((0.5,), (0.5,))\n",
    "                                            \n",
    "                                              ])\n",
    "\n",
    "\n",
    "\n",
    "candi_dataset.transform = trainTransform_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "candi_loader = torch.utils.data.DataLoader(candi_dataset , batch_size=len(candi_dataset), shuffle=False, \n",
    "                               num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 4, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            Reshape((-1,4*8*8)),\n",
    "            nn.Linear(4*8*8, 2*8*8),\n",
    "\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2*8*8, 4*8*8),\n",
    "            nn.LeakyReLU(),\n",
    "            Reshape((-1,4,8,8)),\n",
    "            nn.ConvTranspose2d(4, 16, 2, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  \n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Reshape()\n",
       "    (7): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Reshape()\n",
       "    (3): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoEncoder().to(device)\n",
    "#model.load_state_dict(torch.load('Proposed_AutoEncoder'))\n",
    "model.load_state_dict(torch.load('AutoEncoder'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_list = []\n",
    "\n",
    "for image_batch, person_id_batch in candi_loader : \n",
    "    \n",
    "    imgs_vector = model.encoder(image_batch.to(device))\n",
    "    \n",
    "    for img_vector, person_id in zip(imgs_vector, person_id_batch) :\n",
    "        vector_list.append(img_vector.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "distance_list = []\n",
    "\n",
    "for vector in vector_list[1:] :\n",
    "    distance_list.append(distance.euclidean(vector_list[0], vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.038512229919434, 14.653668403625488, 11.653820037841797, 9.835874557495117]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = person_box[distance_list.index(min(distance_list))]\n",
    "cv2.rectangle(img, (x, y), (x + w, y + h), colors[0], 2)\n",
    "\n",
    "cv2.putText(img, \"answer\", (x, y + 30), font, 3, colors[0], 3)\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
